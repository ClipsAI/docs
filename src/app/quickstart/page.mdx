export const metadata = {
  title: 'Quickstart',
  description:
    'This guide will get you all set up and ready to use the Protocol API. Weâ€™ll cover how to get started an API client and how to make your first API request.',
}

# Quickstart

<Note>
  	Clips AI is currently built for narrative-based content, utilizing video transcripts
    to find clips, and our resizing functionality focuses on reframing the video to the
    current speaker.
</Note>

## Installation

<CodeGroup>

```bash {{ language: 'python' }}
# Install the package
pip install clipsai
```

</CodeGroup>


## Creating clips

Clips are created based on the transcript of a video. You'll first need to transcribe
the video, and then you can create clips based on the transcript. We use 
[WhisperX](https://github.com/m-bain/whisperX) to transcribe videos.

<CodeGroup tag="GET" label="/v1/conversations">

```python
from clipsai import clip, transcribe

transcripiton = transcribe("video.mp4")
clips = clip(transcripiton)
```

</CodeGroup>

<div className="not-prose">
	<Button href="/conversations" variant="text" arrow="right">
    	<>Read clipping documetation</>
  	</Button>
</div>

## Resizing a video

In order to resize a video, you'll need to create an access token on hugging face
because we use [Pyannote](https://github.com/pyannote/pyannote-audio) for speaker 
diarization. You will not be charged for using Pyannote and instructions are on the
[Pyannote](https://huggingface.co/pyannote/speaker-diarization-3.0) HuggingFace page.

<CodeGroup tag="GET" label="/v1/conversations">

```python
from clipsai import resize

crops = resize(
    video_file_path="video.mp4",
    pyannote_auth_token="pyannote_token",
    aspect_ratio=(9, 16)
)
print("Crops: ", crops.segments)
```

</CodeGroup>

<div className="not-prose">
	<Button href="/conversations" variant="text" arrow="right">
    	<>Read resizing documentation</>
  	</Button>
</div>

## References
- [Clipping](#)
- [Resizing](/conversations)
- [Transcribing](/errors)
